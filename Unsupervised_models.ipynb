{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df = pd.read_csv('final_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().any()\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reviews'] = df['reviews'].replace('\\d+?', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_reviews =  df.copy(deep=True)['reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'n_grams'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-cf4db3fe783c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m tf_idf_vectorize = TfidfVectorizer(max_df=0.5, max_features=10000,\n\u001b[0;32m      3\u001b[0m                                   \u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                                   use_idf=True, n_grams=(1,2))\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtf_idf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_idf_vectorize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned_reviews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'n_grams'"
     ]
    }
   ],
   "source": [
    "# vectorize the data and get check what the top words\n",
    "tf_idf_vectorize = TfidfVectorizer(max_df=0.5, max_features=10000,\n",
    "                                  min_df=2, stop_words='english',\n",
    "                                  use_idf=True)\n",
    "\n",
    "tf_idf = tf_idf_vectorize.fit_transform(cleaned_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommended components for LSA\n",
    "SVD = TruncatedSVD(n_components=100, random_state=1234)\n",
    "\n",
    "SVD_matrix = SVD.fit_transform(tf_idf)\n",
    "\n",
    "LSA = Normalizer(copy=False).fit_transform(SVD_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 rows - these are the features created after applying SVD which reduces the dimensionality of our feature space\n",
    "# but maintains\n",
    "LSA_df = pd.DataFrame(SVD.components_, columns=tf_idf_vectorize.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for compNum in range(0, 10):\n",
    "\n",
    "    comp = SVD.components_[compNum]\n",
    "    \n",
    "    # Sort the weights in the first component, and get the indeces\n",
    "    indeces = numpy.argsort(comp).tolist()\n",
    "    \n",
    "    # Reverse the indeces, so we have the largest weights first.\n",
    "    indeces.reverse()\n",
    "    \n",
    "    # Grab the top 10 terms which have the highest weight in this component.        \n",
    "    terms = [features[weightIndex] for weightIndex in indeces[0:10]]    \n",
    "    weights = [comp[weightIndex] for weightIndex in indeces[0:10]]    \n",
    "   \n",
    "    # Display these terms and their weights as a horizontal bar graph.    \n",
    "    # The horizontal bar graph displays the first item on the bottom; reverse\n",
    "    # the order of the terms so the biggest one is on top.\n",
    "    terms.reverse()\n",
    "    weights.reverse()\n",
    "    positions = arange(10) + .5    # the bar centers on the y axis\n",
    "    \n",
    "    figure(compNum)\n",
    "    barh(positions, weights, align='center')\n",
    "    yticks(positions, terms)\n",
    "    xlabel('Weight')\n",
    "    title('Strongest terms for component %d' % (compNum))\n",
    "    grid(True)\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, features, n):\n",
    "    ''' This model prints and saves the 'N' most important words from every topic '''\n",
    "    # make sure the features is in a numpy array to use .argsort\n",
    "    if type(features) == list:\n",
    "        features = np.array(features)\n",
    "    \n",
    "    # save the n most important words for each topic\n",
    "    components = model.components_ \n",
    "    top_n = [features[component.argsort()][-n-1:] for component in components]\n",
    "    \n",
    "    # print the top words for every each topic\n",
    "    for i in range(len(top_n)):\n",
    "        print(f\"Topic {i+1} most important words: {top_n[i]}\")\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 most important words: ['internet' 'signal' 'connection' 'support' 'time' 'unit' 'network'\n",
      " 'device' 'work' 'wireless' 'router']\n",
      "Topic 2 most important words: ['money' 'worked' 'bought' 'time' 'buy' 'item' 'month' 'amazon' 'work'\n",
      " 'battery' 'product']\n",
      "Topic 3 most important words: ['board' 'ram' 'motherboard' 'cpu' 'drive' 'power' 'laptop' 'cooler'\n",
      " 'card' 'case' 'fan']\n",
      "Topic 4 most important words: ['use' 'tablet' 'key' 'like' 'fit' 'screen' 'ipad' 'cover' 'mouse'\n",
      " 'keyboard' 'case']\n",
      "Topic 5 most important words: ['use' 'power' 'mount' 'usb' 'cord' 'plug' 'charger' 'work' 'charge'\n",
      " 'battery' 'cable']\n",
      "Topic 6 most important words: ['noise' 'pair' 'volume' 'headset' 'good' 'quality' 'bass' 'speaker' 'ear'\n",
      " 'headphone' 'sound']\n",
      "Topic 7 most important words: ['device' 'unit' 'play' 'mp' 'use' 'work' 'sound' 'music' 'player' 'ipod'\n",
      " 'radio']\n",
      "Topic 8 most important words: ['picture' 'channel' 'player' 'dvd' 'antenna' 'work' 'monitor' 'remote'\n",
      " 'hdmi' 'cable' 'tv']\n",
      "Topic 9 most important words: ['driver' 'hard' 'software' 'file' 'gb' 'computer' 'window' 'work' 'usb'\n",
      " 'card' 'drive']\n",
      "Topic 10 most important words: ['video' 'great' 'photo' 'use' 'good' 'light' 'flash' 'picture' 'canon'\n",
      " 'lens' 'camera']\n"
     ]
    }
   ],
   "source": [
    "# extract feature name and run LDA model\n",
    "features = tf_idf_vectorize.get_feature_names()\n",
    "LDA = LatentDirichletAllocation(n_components=10, random_state=1234,\n",
    "                                n_jobs=-1, verbose=1).fit(tf_idf)\n",
    "\n",
    "LDA_topics = print_topics(LDA, features , 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf bigrams\n",
    "tf_idf_n2_vec = TfidfVectorizer(max_df=0.5, max_features=8000,\n",
    "                                min_df=2, stop_words='english',\n",
    "                                use_idf=True, ngram_range=(1, 2))\n",
    "# extract features name \n",
    "features_n2 = tf_idf_n2_vec.get_feature_names()\n",
    "\n",
    "# fit transform\n",
    "tf_idf_n2 = tf_idf_n2_vec.fit_transform(cleaned_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n",
      "Topic 1 most important words: ['good' 'nice' 'small' 'little' 'strap' 'like' 'laptop' 'fan' 'fit' 'bag'\n",
      " 'case']\n",
      "Topic 2 most important words: ['internet' 'support' 'wifi' 'modem' 'device' 'signal' 'connection' 'work'\n",
      " 'network' 'wireless' 'router']\n",
      "Topic 3 most important words: ['software' 'memory' 'file' 'hard drive' 'work' 'window' 'gb' 'computer'\n",
      " 'usb' 'card' 'drive']\n",
      "Topic 4 most important words: ['use' 'like' 'quality' 'headset' 'key' 'good' 'keyboard' 'ear' 'sound'\n",
      " 'headphone' 'mouse']\n",
      "Topic 5 most important words: ['audio' 'player' 'cable' 'hdmi' 'good' 'monitor' 'quality' 'great' 'tv'\n",
      " 'sound' 'speaker']\n",
      "Topic 6 most important words: ['gps' 'channel' 'time' 'antenna' 'use' 'button' 'work' 'device' 'unit'\n",
      " 'tv' 'remote']\n",
      "Topic 7 most important words: ['quality' 'flash' 'light' 'use' 'good' 'protector' 'picture' 'canon'\n",
      " 'screen' 'lens' 'camera']\n",
      "Topic 8 most important words: ['fit' 'great' 'work' 'kindle' 'charger' 'tablet' 'cover' 'charge' 'ipad'\n",
      " 'battery' 'case']\n",
      "Topic 9 most important words: ['return' 'time' 'money' 'buy' 'item' 'worked' 'month' 'amazon' 'work'\n",
      " 'product' 'cable']\n",
      "Topic 10 most important words: ['cord' 'good' 'use' 'player' 'cable' 'car' 'wire' 'work' 'radio' 'mount'\n",
      " 'ipod']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array(['good', 'nice', 'small', 'little', 'strap', 'like', 'laptop',\n",
       "        'fan', 'fit', 'bag', 'case'], dtype='<U25'),\n",
       " array(['internet', 'support', 'wifi', 'modem', 'device', 'signal',\n",
       "        'connection', 'work', 'network', 'wireless', 'router'],\n",
       "       dtype='<U25'),\n",
       " array(['software', 'memory', 'file', 'hard drive', 'work', 'window', 'gb',\n",
       "        'computer', 'usb', 'card', 'drive'], dtype='<U25'),\n",
       " array(['use', 'like', 'quality', 'headset', 'key', 'good', 'keyboard',\n",
       "        'ear', 'sound', 'headphone', 'mouse'], dtype='<U25'),\n",
       " array(['audio', 'player', 'cable', 'hdmi', 'good', 'monitor', 'quality',\n",
       "        'great', 'tv', 'sound', 'speaker'], dtype='<U25'),\n",
       " array(['gps', 'channel', 'time', 'antenna', 'use', 'button', 'work',\n",
       "        'device', 'unit', 'tv', 'remote'], dtype='<U25'),\n",
       " array(['quality', 'flash', 'light', 'use', 'good', 'protector', 'picture',\n",
       "        'canon', 'screen', 'lens', 'camera'], dtype='<U25'),\n",
       " array(['fit', 'great', 'work', 'kindle', 'charger', 'tablet', 'cover',\n",
       "        'charge', 'ipad', 'battery', 'case'], dtype='<U25'),\n",
       " array(['return', 'time', 'money', 'buy', 'item', 'worked', 'month',\n",
       "        'amazon', 'work', 'product', 'cable'], dtype='<U25'),\n",
       " array(['cord', 'good', 'use', 'player', 'cable', 'car', 'wire', 'work',\n",
       "        'radio', 'mount', 'ipod'], dtype='<U25')]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA_n2 = LatentDirichletAllocation(n_components=10, random_state=1234,\n",
    "                                   n_jobs=-1, verbose=1).fit(tf_idf_n2)\n",
    "\n",
    "LDA_n2_topics = print_topics(LDA_n2, features_n2 , 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 most important words: ['set' 'use' 'player' 'problem' 'remote' 'screen' 'time' 'router' 'device'\n",
      " 'unit' 'tv']\n",
      "Topic 2 most important words: ['zoom' 'bag' 'video' 'photo' 'image' 'quality' 'mm' 'canon' 'picture'\n",
      " 'lens' 'camera']\n",
      "Topic 3 most important words: ['better' 'price' 'volume' 'music' 'bass' 'good' 'quality' 'ear' 'speaker'\n",
      " 'headphone' 'sound']\n",
      "Topic 4 most important words: ['stand' 'hold' 'nice' 'like' 'screen' 'tablet' 'kindle' 'fit' 'cover'\n",
      " 'ipad' 'case']\n",
      "Topic 5 most important words: ['time' 'phone' 'power' 'original' 'hour' 'charged' 'charging' 'life'\n",
      " 'charger' 'charge' 'battery']\n",
      "Topic 6 most important words: ['connection' 'audio' 'end' 'port' 'plug' 'quality' 'tv' 'connector' 'usb'\n",
      " 'hdmi' 'cable']\n",
      "Topic 7 most important words: ['port' 'seagate' 'backup' 'data' 'gb' 'external' 'file' 'computer' 'hard'\n",
      " 'usb' 'drive']\n",
      "Topic 8 most important words: ['wireless' 'laptop' 'like' 'feel' 'usb' 'use' 'button' 'logitech' 'key'\n",
      " 'keyboard' 'mouse']\n",
      "Topic 9 most important words: ['class' 'computer' 'speed' 'usb' 'video' 'sandisk' 'reader' 'gb' 'memory'\n",
      " 'sd' 'card']\n",
      "Topic 10 most important words: ['worked' 'recommend' 'bought' 'item' 'buy' 'fine' 'good' 'price'\n",
      " 'product' 'great' 'work']\n"
     ]
    }
   ],
   "source": [
    "# NMF no grams model\n",
    "nmf = NMF(n_components=10, random_state=123, alpha=.1, l1_ratio=.5).fit(tf_idf)\n",
    "nmf_topics = print_topics(nmf, features , 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 most important words: ['remote' 'great' 'problem' 'use' 'router' 'time' 'device' 'unit' 'tv'\n",
      " 'product' 'work']\n",
      "Topic 2 most important words: ['shoot' 'zoom' 'shot' 'image' 'quality' 'flash' 'photo' 'canon' 'video'\n",
      " 'picture' 'camera']\n",
      "Topic 3 most important words: ['price' 'volume' 'music' 'sound quality' 'bass' 'good' 'quality' 'ear'\n",
      " 'speaker' 'headphone' 'sound']\n",
      "Topic 4 most important words: ['hold' 'stand' 'protector' 'like' 'tablet' 'screen' 'kindle' 'fit'\n",
      " 'cover' 'ipad' 'case']\n",
      "Topic 5 most important words: ['seagate' 'backup' 'data' 'external' 'computer' 'gb' 'file' 'hard' 'usb'\n",
      " 'hard drive' 'drive']\n",
      "Topic 6 most important words: ['quality' 'port' 'plug' 'connector' 'work' 'tv' 'cable work' 'hdmi cable'\n",
      " 'usb' 'hdmi' 'cable']\n",
      "Topic 7 most important words: ['phone' 'original' 'power' 'hour' 'charged' 'battery life' 'charging'\n",
      " 'life' 'charger' 'charge' 'battery']\n",
      "Topic 8 most important words: ['hand' 'laptop' 'like' 'feel' 'usb' 'use' 'button' 'logitech' 'key'\n",
      " 'keyboard' 'mouse']\n",
      "Topic 9 most important words: ['speed' 'usb' 'sandisk' 'card reader' 'memory card' 'reader' 'gb'\n",
      " 'memory' 'sd card' 'sd' 'card']\n",
      "Topic 10 most important words: ['mm lens' 'zoom' 'cap' 'nikon' 'bag' 'hood' 'focus' 'canon' 'filter' 'mm'\n",
      " 'lens']\n"
     ]
    }
   ],
   "source": [
    "# NMF bigrams model\n",
    "nmf_n2 = NMF(n_components=10, random_state=123, alpha=.1, l1_ratio=.5).fit(tf_idf_n2)\n",
    "nmf2_topics = print_topics(nmf_n2, features_n2 , 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify the reviews into different topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "tokened_reviews = [word_tokenize(i) for i in cleaned_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(tokened_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=10, no_above=0.5, keep_n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in tokened_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.022*\"camera\" + 0.012*\"lens\" + 0.009*\"battery\" + 0.007*\"canon\" + 0.006*\"picture\" + 0.006*\"d\" + 0.005*\"nikon\" + 0.005*\"video\" + 0.005*\"photo\" + 0.005*\"image\"\n",
      "Topic: 1 Word: 0.021*\"drive\" + 0.013*\"card\" + 0.010*\"usb\" + 0.008*\"gb\" + 0.006*\"window\" + 0.006*\"file\" + 0.005*\"speed\" + 0.005*\"computer\" + 0.005*\"ssd\" + 0.005*\"hard\"\n",
      "Topic: 2 Word: 0.018*\"sound\" + 0.013*\"headphone\" + 0.013*\"speaker\" + 0.011*\"ear\" + 0.007*\"bass\" + 0.006*\"quality\" + 0.006*\"headset\" + 0.006*\"volume\" + 0.005*\"good\" + 0.005*\"music\"\n",
      "Topic: 3 Word: 0.017*\"protector\" + 0.015*\"screen\" + 0.011*\"bubble\" + 0.008*\"fan\" + 0.005*\"dust\" + 0.004*\"case\" + 0.004*\"air\" + 0.004*\"sata\" + 0.004*\"one\" + 0.004*\"product\"\n",
      "Topic: 4 Word: 0.020*\"keyboard\" + 0.013*\"mouse\" + 0.009*\"key\" + 0.006*\"tablet\" + 0.006*\"button\" + 0.004*\"use\" + 0.004*\"screen\" + 0.004*\"typing\" + 0.004*\"bluetooth\" + 0.004*\"touch\"\n",
      "Topic: 5 Word: 0.009*\"charge\" + 0.007*\"phone\" + 0.006*\"battery\" + 0.006*\"charger\" + 0.006*\"iphone\" + 0.006*\"radio\" + 0.005*\"charging\" + 0.005*\"device\" + 0.004*\"car\" + 0.004*\"unit\"\n",
      "Topic: 6 Word: 0.007*\"router\" + 0.004*\"device\" + 0.004*\"work\" + 0.004*\"product\" + 0.004*\"support\" + 0.003*\"wifi\" + 0.003*\"amazon\" + 0.003*\"one\" + 0.003*\"time\" + 0.003*\"network\"\n",
      "Topic: 7 Word: 0.021*\"case\" + 0.012*\"cover\" + 0.011*\"ipad\" + 0.009*\"kindle\" + 0.008*\"fit\" + 0.008*\"tablet\" + 0.005*\"stand\" + 0.005*\"like\" + 0.005*\"well\" + 0.004*\"protection\"\n",
      "Topic: 8 Word: 0.019*\"tv\" + 0.011*\"cable\" + 0.009*\"hdmi\" + 0.006*\"monitor\" + 0.006*\"roku\" + 0.006*\"remote\" + 0.006*\"work\" + 0.005*\"player\" + 0.005*\"netflix\" + 0.005*\"great\"\n",
      "Topic: 9 Word: 0.007*\"antenna\" + 0.007*\"mount\" + 0.005*\"cable\" + 0.005*\"screw\" + 0.004*\"wall\" + 0.004*\"plug\" + 0.003*\"wire\" + 0.003*\"cord\" + 0.003*\"work\" + 0.003*\"connector\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topics = pd.DataFrame(LDA.transform(td_idf))\n",
    "topic_column_names = [\"topic_{}\".format(c) for c in doc_topics.columns]\n",
    "doc_topics.columns = topic_column_names\n",
    "sample_with_topics = pd.concat([df, doc_topics], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
